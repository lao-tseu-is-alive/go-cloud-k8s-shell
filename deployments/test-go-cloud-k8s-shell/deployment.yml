---
### TEMPLATE FOR A SERVICE AND DEPLOYMENT ON KUBERNETES just substitute the next keywords with your own values
### go-cloud-k8s-shell     : the name of your application or container image
### v0.1.7  : the version of the app image to deploy
### ghcr.io/lao-tseu-is-alive  : the prefix to your images in your container registry
apiVersion: v1
kind: Namespace
metadata:
  name: test-go-cloud-k8s-shell
  labels:
    env: test
    app: go-cloud-k8s-shell
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
  namespace: test-go-cloud-k8s-shell
  labels:
    app: go-cloud-k8s-shell
spec:
  hard:
    requests.cpu: "1"
    requests.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: test-go-cloud-k8s-shell
  name: pod-reader-role
  labels:
    app: go-cloud-k8s-shell
rules:
  - apiGroups: [""]  # "" indicates the core API group
    resources: ["pods", "namespaces"]
    verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-reader-role-binding
  labels:
    app: go-cloud-k8s-shell
subjects:
  - kind: Group
    name: system:serviceaccounts
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: service-reader-role
  labels:
    app: go-cloud-k8s-shell
rules:
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "watch", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: service-reader-role-binding
  labels:
    app: go-cloud-k8s-shell
subjects:
  - kind: Group
    name: system:serviceaccounts
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: service-reader-role
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Service                    # Type of kubernetes resource
metadata:
  name: go-cloud-k8s-shell-service   # Name of the resource
  namespace: test-go-cloud-k8s-shell
  labels:     # The labels that will be applied
    app: go-cloud-k8s-shell
  annotations:
    externalTrafficPolicy: "local"
spec:
  #type: NodePort                 # A port is opened on each node in your cluster via Kube proxy.
  # https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer
  type: LoadBalancer             # Adds a load balancer (from the cloud provider)
  # k3s will deploy a daemonset listening on the given port on the host node
  # By default k3s have a builtin load balancer called "klipper" https://github.com/k3s-io/klipper-lb ok for one dev node.
  # It is possible to run multiple Services on the same node, as long as they use different ports.
  # in case you are using a 'real' k3s cluster with multiple nodes consider using metalLB instead
  externalTrafficPolicy: Local
  loadBalancerSourceRanges:
    - 192.168.0.0/16
  ports:                         # Take incoming HTTP requests on this port (exposed) and forward them to the targetPort of container
    - name: http
      port: 9999
      targetPort: 9999            # Should match the PORT env variable in deployment and containerPort that the Go application listens on
  selector:
    app: go-cloud-k8s-shell         # Map any pod with this label `app=go-cloud-k8s-shell` to this service
---
apiVersion: apps/v1
kind: Deployment                 # Type of Kubernetes resource
metadata:
  name: go-cloud-k8s-shell           # Name of the Kubernetes resource
  namespace: test-go-cloud-k8s-shell
  labels:
    app: go-cloud-k8s-shell
spec:
  replicas: 2                    # Number of pods to run at any given time
  selector:
    matchLabels:
      app: go-cloud-k8s-shell              # This deployment applies to any Pods matching the specified label
  template:                      # This deployment will create a set of pods using the configurations in this template
    metadata:
      labels:                    # The labels that will be applied to all the pods in this deployment
        app: go-cloud-k8s-shell
    spec:                        # Spec for the container which will run in the Pod
      securityContext:           # https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
        runAsUser: 1221           # using a non privileged user
        runAsGroup: 1221          # using the group 'gouser' created in Dockerfile
        fsGroup: 100             # by adding fsGroup field, all processes of the container are also part
                                 # of the supplementary group ID 100 (users).
        supplementalGroups:
          - 100
      containers:
      - name: go-cloud-k8s-shell
        image: ghcr.io/lao-tseu-is-alive/go-cloud-k8s-shell:v0.1.7
        imagePullPolicy: Always
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1221           # using the user 'gouser' created in Dockerfile
          runAsGroup: 1221          # using the group 'gouser' created in Dockerfile
          capabilities:
            drop:
              - 'ALL'
          readOnlyRootFilesystem: true
        ports:
          - containerPort: 9999  # Should match the PORT env variable above  that the Go application listens on
        resources:
          limits:            # resource limit imposed to the pod, the container cannot utilize more res than specified
            cpu: 1000m       # 1000 millicpu or millicores 1 or 100% of a CPU core of a running node
            memory: 128Mi
          requests:          # explicit request of the minimum amount of resources the pod need
            cpu: 100m        # 100 millicpu or millicores 0.1 or 10% of a CPU core of a running node
            memory: 32Mi
        livenessProbe:           # To check the health of the Pod
          httpGet:
            path: /health
            port: 9999
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 15
          timeoutSeconds: 2
        readinessProbe:          # To check if the Pod is ready to serve traffic or not
          httpGet:
            path: /readiness
            port: 9999
            scheme: HTTP
          initialDelaySeconds: 5
          timeoutSeconds: 2
        env:
          - name: PORT
            value: "9999"
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                fieldPath: metadata.namespace
          - name: MY_POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: MY_POD_SERVICE_ACCOUNT
            valueFrom:
              fieldRef:
                fieldPath: spec.serviceAccountName
